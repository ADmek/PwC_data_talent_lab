# -*- coding: utf-8 -*-
"""Kopia notatnika PwC_acceleration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1opB-Qw9wn1V00YdQNwys5_lqXDY4sgnj
"""

import numpy as np 
import pandas as pd 
import seaborn as sns

"""# Wgrywanie i czyszczenie danych"""

df = pd.read_csv('acceleration_data.csv', skipinitialspace=True,sep=',""', engine ='python',doublequote=False)
df.head(3)

df = pd.read_csv('acceleration_data.csv', skipinitialspace=True,sep=',""', engine ='python',doublequote=False).replace('"','',regex=True)
df.head(3)

df_czysty = df.rename(columns={'"ID':'ID', 'GEO""':'GEO', 'SECTOR""':'SECTOR','EMPLOYEES""':'EMPLOYEES','INCOME""':'INCOME','MARKETING_SPENDING""':'MARKETING_SPENDING','ACC_BEFORE""':'ACC_BEFORE','PWC_PRESS_INDEX""':'PWC_PRESS_INDEX','FB_LIKES""':'FB_LIKES','TWT_FOLLOWERS""':'TWT_FOLLOWERS','YEAR_FOUNDED""':'YEAR_FOUNDED','EXP_CEO""':'EXP_CEO','AREA""':'AREA','PWC_EMPLOYEES""':'PWC_EMPLOYEES','CREDIT""':'CREDIT','FLAG"""':'FLAG'})
df_czysty.head(3)

df_czysty.columns

df_czysty_typydanych = df_czysty.astype({'ID':int, 'GEO':object, 'SECTOR':str, 'EMPLOYEES':int, 'INCOME':float, 'MARKETING_SPENDING':float,
       'ACC_BEFORE':int, 'PWC_PRESS_INDEX':float, 'FB_LIKES':int, 'TWT_FOLLOWERS':int,
       'YEAR_FOUNDED':object, 'EXP_CEO':object, 'AREA':float, 'PWC_EMPLOYEES':int, 'CREDIT':float, 'FLAG':int})

df_czysty_typydanych.dtypes

"""# Geospacial data

rozdzielamy kolumnę zawierającą długość i szerokość geograficzną na dwie
"""

lat = []
lon = []

for row in df_czysty_typydanych['GEO']:
    # Try to,
    try:
        # Split the row by comma and append
        # everything before the comma to lat
        lat.append(row.split(',')[0])
        # Split the row by comma and append
        # everything after the comma to lon
        lon.append(row.split(',')[1])
    except:
        # append a missing value to lat
        lat.append(np.NaN)
        # append a missing value to lon
        lon.append(np.NaN)


# Create two new columns from lat and lon
df_czysty_typydanych['latitude'] = lat
df_czysty_typydanych['longitude'] = lon
df_czysty_typydanych.drop('GEO', axis = 1)

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

K_clusters = range(1,10)
kmeans = [KMeans(n_clusters=i) for i in K_clusters]
Y_axis = df_czysty_typydanych[['latitude']]
X_axis = df_czysty_typydanych[['longitude']]
score = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]
# Visualize
plt.plot(K_clusters, score)
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Curve')
plt.show()

"""Wniosek - że podzielimy dane na 3 clusters"""

km = KMeans(n_clusters = 3, random_state=45)

y_predicted = km.fit_predict(df_czysty_typydanych[['latitude', 'longitude']])
y_predicted.shape

df_czysty_typydanych['cluster_label'] = y_predicted
df_czysty_typydanych.head(2)

"""# Exploratory data analysis

Sprawdzam ile firm z bazy danych upadło a ile nie.
Wygląda to na w dobrze zbalasnowaną bazę danych
"""

df_clean = df_czysty_typydanych
sns.histplot(df_clean['FLAG'])

"""Robię pierwsze korelacje i wizualizacje"""

df_clean.columns

sns.heatmap(df_clean.corr())

"""Dzielę bazę danych na dwie. Te firmy, które upadły w ciągu dwóch lat i te które nie upadły.

"""

grupowanie = df_clean.groupby(df_czysty.FLAG)
df_upadli = grupowanie.get_group('1')
df_nieupadli = grupowanie.get_group('0')

plt.hist(df_upadli['EMPLOYEES'], bins = 30)
plt.show()

plt.hist(df_nieupadli['EMPLOYEES'], bins = 30)
plt.show()

plt.hist(df_upadli['MARKETING_SPENDING'], bins = 30)
plt.show()

plt.hist(df_nieupadli['MARKETING_SPENDING'], bins = 30)
plt.show()

grupowanie_sektor = df_clean.groupby(df_czysty_typydanych.SECTOR)
df_IT = grupowanie_sektor.get_group('IT')
df_RESTAURANT = grupowanie_sektor.get_group('RESTAURANT')
df_PUB = grupowanie_sektor.get_group('PUB')

plt.hist(df_IT["EMPLOYEES"], bins = 30)
plt.show()

df_IT.loc[df_IT['FLAG']==1].describe()

df_IT.loc[df_IT['FLAG']==0].describe()

df_RESTAURANT.loc[df_RESTAURANT['FLAG']==1].describe()

df_RESTAURANT.loc[df_RESTAURANT['FLAG']==0].describe()

df_PUB.loc[df_PUB['FLAG']==1].describe()

df_PUB.loc[df_PUB['FLAG']==0].describe()

plt.hist(df_RESTAURANT["EMPLOYEES"], bins = 30)
plt.show()

from matplotlib import pyplot
#bins = np.linspace(-10, 10, 100)
pyplot.hist(df_nieupadli['EMPLOYEES'],bins=30, alpha=0.5, label='nieupadli')
pyplot.hist(df_upadli['EMPLOYEES'],bins=30, alpha=0.5, label='upadli')
pyplot.legend(loc='upper right')
pyplot.show()

from matplotlib import pyplot
#bins = np.linspace(-10, 10, 100)
pyplot.hist(df_nieupadli['MARKETING_SPENDING'],bins=30, alpha=0.5, label='nieupadli')
pyplot.hist(df_upadli['MARKETING_SPENDING'],bins=30, alpha=0.5, label='upadli')
pyplot.legend(loc='upper right')
pyplot.show()

df_upadli.shape

df_nieupadli.shape

df_upadli.describe()

df_nieupadli.describe()

"""Sprwadzam brakujące dane"""

sns.heatmap(df_clean.isnull(), yticklabels = False, cbar = False, cmap = 'viridis')

"""Nie ma braków  w danych"""

df_clean['SECTOR'].value_counts()

df_clean['EXP_CEO'].value_counts()

df_clean['EMPLOYEES'].value_counts().head()

df_clean['ACC_BEFORE'].value_counts()

df_clean['YEAR_FOUNDED'].value_counts()

type(df_clean['EXP_CEO'])

"""Sprawdzam różnice między grupami (ttest i wskaźnik siły efektu)"""

from scipy.stats import ttest_ind
from scipy.stats import levene
lev = levene(df_upadli['EMPLOYEES'],df_nieupadli['EMPLOYEES'])
ttes = ttest_ind(df_upadli['EMPLOYEES'],df_nieupadli['EMPLOYEES'], equal_var = False)
print("wynik testu levena to",lev, "a wynik testu t to ", ttes)

from statistics import mean, stdev
from math import sqrt

cohens_d = (mean(df_upadli['EMPLOYEES']) - mean(df_nieupadli['EMPLOYEES'])) / (sqrt((stdev(df_upadli['EMPLOYEES']) ** 2 + stdev(df_nieupadli['EMPLOYEES']) ** 2) / 2))

print(cohens_d)

lev = levene(df_upadli['INCOME'],df_nieupadli['INCOME'])
ttes = ttest_ind(df_upadli['INCOME'],df_nieupadli['INCOME'], equal_var = False)
print("wynik testu levena to",lev, "a wynik testu t to ", ttes)

cohens_d = (mean(df_upadli['INCOME']) - mean(df_nieupadli['INCOME'])) / (sqrt((stdev(df_upadli['INCOME']) ** 2 + stdev(df_nieupadli['INCOME']) ** 2) / 2))

print(cohens_d)

lev = levene(df_upadli['MARKETING_SPENDING'],df_nieupadli['MARKETING_SPENDING'])
ttes = ttest_ind(df_upadli['MARKETING_SPENDING'],df_nieupadli['MARKETING_SPENDING'], equal_var = False)
print("wynik testu levena to",lev, "a wynik testu t to ", ttes)

cohens_d = (mean(df_upadli['MARKETING_SPENDING']) - mean(df_nieupadli['MARKETING_SPENDING'])) / (sqrt((stdev(df_upadli['MARKETING_SPENDING']) ** 2 + stdev(df_nieupadli['MARKETING_SPENDING']) ** 2) / 2))

print(cohens_d)

lev = levene(df_upadli['PWC_PRESS_INDEX'],df_nieupadli['PWC_PRESS_INDEX'])
ttes = ttest_ind(df_upadli['PWC_PRESS_INDEX'],df_nieupadli['PWC_PRESS_INDEX'], equal_var = False)
print("wynik testu levena to",lev, "a wynik testu t to ", ttes)

cohens_d = (mean(df_upadli['PWC_PRESS_INDEX']) - mean(df_nieupadli['PWC_PRESS_INDEX'])) / (sqrt((stdev(df_upadli['PWC_PRESS_INDEX']) ** 2 + stdev(df_nieupadli['PWC_PRESS_INDEX']) ** 2) / 2))

print(cohens_d)

lev = levene(df_upadli['TWT_FOLLOWERS'],df_nieupadli['TWT_FOLLOWERS'])
ttes = ttest_ind(df_upadli['TWT_FOLLOWERS'],df_nieupadli['TWT_FOLLOWERS'], equal_var = False)
print("wynik testu levena to",lev, "a wynik testu t to ", ttes)

cohens_d = (mean(df_upadli['TWT_FOLLOWERS']) - mean(df_nieupadli['TWT_FOLLOWERS'])) / (sqrt((stdev(df_upadli['TWT_FOLLOWERS']) ** 2 + stdev(df_nieupadli['TWT_FOLLOWERS']) ** 2) / 2))

print(cohens_d)

lev = levene(df_upadli['AREA'],df_nieupadli['AREA'])
ttes = ttest_ind(df_upadli['AREA'],df_nieupadli['AREA'], equal_var = False)
print("wynik testu levena to",lev, "a wynik testu t to ", ttes)

cohens_d = (mean(df_upadli['AREA']) - mean(df_nieupadli['AREA'])) / (sqrt((stdev(df_upadli['AREA']) ** 2 + stdev(df_nieupadli['AREA']) ** 2) / 2))

print(cohens_d)

lev = levene(df_upadli['FB_LIKES'],df_nieupadli['FB_LIKES'])
ttes = ttest_ind(df_upadli['FB_LIKES'],df_nieupadli['FB_LIKES'], equal_var = False)
print("wynik testu levena to",lev, "a wynik testu t to ", ttes)

cohens_d = (mean(df_upadli['FB_LIKES']) - mean(df_nieupadli['FB_LIKES'])) / (sqrt((stdev(df_upadli['FB_LIKES']) ** 2 + stdev(df_nieupadli['FB_LIKES']) ** 2) / 2))

print(cohens_d)

lev = levene(df_upadli['PWC_EMPLOYEES'],df_nieupadli['PWC_EMPLOYEES'])
ttes = ttest_ind(df_upadli['PWC_EMPLOYEES'],df_nieupadli['PWC_EMPLOYEES'], equal_var = False)
print("wynik testu levena to",lev, "a wynik testu t to ", ttes)

cohens_d = (mean(df_upadli['PWC_EMPLOYEES']) - mean(df_nieupadli['PWC_EMPLOYEES'])) / (sqrt((stdev(df_upadli['PWC_EMPLOYEES']) ** 2 + stdev(df_nieupadli['PWC_EMPLOYEES']) ** 2) / 2))

print(cohens_d)

lev = levene(df_upadli['CREDIT'],df_nieupadli['CREDIT'])
ttes = ttest_ind(df_upadli['CREDIT'],df_nieupadli['CREDIT'], equal_var = False)
print("wynik testu levena to",lev, "a wynik testu t to ", ttes)

cohens_d = (mean(df_upadli['CREDIT']) - mean(df_nieupadli['CREDIT'])) / (sqrt((stdev(df_upadli['CREDIT']) ** 2 + stdev(df_nieupadli['CREDIT']) ** 2) / 2))

print(cohens_d)

"""WIZUALIZACJA zmiennych, które najbardziej różniły się w grupach upadli i nie_upadli (tych zmiennych które później zostały użyte do budowy modelu)"""

sns.kdeplot(df_clean.MARKETING_SPENDING[df_clean.FLAG==0], label='nieupadli', shade=True, color = 'green')
sns.kdeplot(df_clean.MARKETING_SPENDING[df_clean.FLAG==1], label='upadli', shade=True, color = 'red')
plt.legend(labels=['non_Bankrupt', 'bankrupt'])
plt.xlabel('MARKETING SPENDING');

sns.kdeplot(df_clean.EMPLOYEES[df_clean.FLAG==0], label='nieupadli', shade=True, color = 'green')
sns.kdeplot(df_clean.EMPLOYEES[df_clean.FLAG==1], label='upadli', shade=True, color = 'red')
plt.legend(labels=['non_Bankrupt', 'bankrupt'])
plt.xlabel('EMPLOYEES');

sns.kdeplot(df_clean.TWT_FOLLOWERS[df_clean.FLAG==0], label='nieupadli', shade=True, color = 'green')
sns.kdeplot(df_clean.TWT_FOLLOWERS[df_clean.FLAG==1], label='upadli',  shade=True, color = 'red')
plt.legend(labels=['non_Bankrupt', 'bankrupt'])
plt.xlabel('TWITTER FOLLOWERS');

sns.kdeplot(df_clean.PWC_EMPLOYEES[df_clean.FLAG==0], label='nieupadli', shade=True, color = 'green')
sns.kdeplot(df_clean.PWC_EMPLOYEES[df_clean.FLAG==1], label='upadli',  shade=True, color = 'red')
plt.legend(labels=['Nieupadli', 'Upadli'])
plt.xlabel('PWC employees');

"""# Przygotowywanie danych do Machine learning model

Pamiętaj o cleaning data (nie wiem czy zrobiłes to dostatecznie dobrze) , oraz categorical variables (zamieniamy je na tzw. dummy variables)

"""

df_clean['EXP_CEO'].replace(to_replace='NA', value = 7, inplace =True) # 7 to wartość oczekiwana
df_clean['EXP_CEO'].tail(5)

nowakolumna = df_clean['YEAR_FOUNDED'].astype(int).subtract(2014)
YEAR_FOUNDED_ABSOLUTE=np.absolute(nowakolumna)
YEAR_FOUNDED_ABSOLUTE

SECTOR_dummy = pd.get_dummies(df_clean['SECTOR'], drop_first=True)
 SECTOR_dummy

df_clean = pd.concat([df_clean, SECTOR_dummy, YEAR_FOUNDED_ABSOLUTE], axis = 1)
df_clean.head()

df_clean.columns

cols = []
count = 1
for column in df_clean.columns:
    if column == 'YEAR_FOUNDED':
        cols.append(f'YEAR_FOUNDED_{count}')
        count+=1
        continue
    cols.append(column)
df_clean.columns = cols

df_clean.columns

df_clean.drop(['TWT_FOLLOWERS', 'AREA','INCOME','FB_LIKES','PWC_EMPLOYEES','ACC_BEFORE',"CREDIT",'cluster_label','PWC_PRESS_INDEX','ID','IT','PUB','RESTAURANT', 'GEO', 'SECTOR', 'YEAR_FOUNDED_1','latitude', 'longitude' , 'EXP_CEO'], axis = 1, inplace=True )
df_clean.head()

X = df_clean.drop('FLAG', axis=1)
y = df_clean['FLAG']

"""# Model deployment and performance"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30, random_state=48)

from sklearn.linear_model import LogisticRegression

logmodel = LogisticRegression(solver = "liblinear", C=1,random_state = 0 )

logmodel.fit(X_train, y_train)

predictions = logmodel.predict(X_test)

from sklearn.metrics import classification_report

"""Na dole wskaźnik accuracy - czyli jaki procent firm został zakwalifikowany do odpowiedniej kategorii"""

logmodel.score(X_test, y_test)

print(classification_report(y_test, predictions))

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, predictions)

"""Ta linijka kodu pobiera dane ze zbioru testowego i zwraca dla niego prawdopodobieństwo że wartość jest 0 (w pierwszej kolumnie), czyli że firma nie upadnie, w drugiej kolumnie zwracane jest prawdopodobieństwo że firma upadnie)"""

predictions_prob = logmodel.predict_proba(X_test[0:5])
predictions_prob

predictions_1 = logmodel.predict(X_test[0:5])
predictions_1

"""Sprwadzam jakie sa parametry stworzonego modelu. """

logmodel.intercept_

logmodel.coef_

X_train.columns

"""# Wykresy i cohen dla zmiennej YEAR_FOUNDED_2 czyli dla AGE OF THE COMPANY"""

grupowanie = df_clean.groupby(df_czysty.FLAG)
df_upadli = grupowanie.get_group('1')
df_nieupadli = grupowanie.get_group('0')
df_upadli

from matplotlib import pyplot
#bins = np.linspace(-10, 10, 100)
pyplot.hist(df_nieupadli['YEAR_FOUNDED_2'],bins=30, alpha=0.6, label='nieupadli')
pyplot.hist(df_upadli['YEAR_FOUNDED_2'],bins=30, alpha=0.6, label='upadli')
pyplot.legend(loc='upper right')
pyplot.show()

sns.kdeplot(df_clean.YEAR_FOUNDED_2[df_clean.FLAG==0], label='nieupadli', shade=True, color = 'green')
sns.kdeplot(df_clean.YEAR_FOUNDED_2[df_clean.FLAG==1], label='upadli',  shade=True, color = 'red')
plt.legend(labels=['non_Bankrupt', 'bankrupt'])
plt.xlabel('AGE OF THE COMPANY');

df_upadli.describe()

df_nieupadli.describe()

cohens_d = (mean(df_upadli['YEAR_FOUNDED_2']) - mean(df_nieupadli['YEAR_FOUNDED_2'])) / (sqrt((stdev(df_upadli['YEAR_FOUNDED_2']) ** 2 + stdev(df_nieupadli['YEAR_FOUNDED_2']) ** 2) / 2))

print(cohens_d)